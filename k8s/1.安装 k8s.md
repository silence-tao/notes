# 1.环境准备

## 1.1 主机规划

> 一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令
>
> 每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)
>
> 2 CPU 核或更多
>
> 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)

| 角色   | IP 地址       | 操作系统                             | 配置                        |
| ------ | ------------- | ------------------------------------ | --------------------------- |
| master | 10.252.74.100 | CentOS Linux release 7.9.2009 (Core) | 2 核 CPU，4G 内存，50G 硬盘 |
| node-1 | 10.252.74.101 | CentOS Linux release 7.9.2009 (Core) | 2 核 CPU，4G 内存，50G 硬盘 |

## 1.2 环境查看

通过命令 `uname -r` 查看系统内核环境，系统环境必须是 3.10 以上的

``` shell
$ uname -r
3.10.0-862.el7.x86_64
```
通过命令 `cat /etc/os-release` 查看系统属性
``` shell
$ cat /etc/os-release 
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"
```

通过命令 `cat /etc/redhat-release` 查看系统版本（要求操作系统的版本至少在7.5以上）

``` bash
$ cat /etc/redhat-release
CentOS Linux release 7.9.2009 (Core)
```
## 1.3 环境搭建

### 1.3.1 关闭防火墙

``` shell
# 查看防火墙状态
$ firewall-cmd --state

# 关闭防火墙
$ systemctl stop firewalld

# 禁止防火墙开机自启动
$ systemctl disable firewalld
Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.
Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.
```

### 1.3.2 主机名解析

为了方便后面集群节点间的直接调用，需要配置一下主机名解析，企业中推荐使用内部的DNS服务器。

```shell
$ cat >> /etc/hosts << EOF
10.252.74.48 master
10.252.74.49 node-1

# github 的一些域名的 DNS 解析被污染，会导致 DNS 解析过程无法通过域名取得正确的IP地址
# 所以这里添加域名的绑定
199.232.68.133 raw.githubusercontent.com
199.232.68.133 user-images.githubusercontent.com
199.232.68.133 avatars2.githubusercontent.com
199.232.68.133 avatars1.githubusercontent.com
EOF
```

### 1.3.3 时间同步

kubernetes要求集群中的节点时间必须精确一致，所以在每个节点上添加时间同步：

```shell
# 启动 chronyd 服务
$ systemctl start chronyd

# 设置 chronyd 开机自启
$ systemctl enable chronyd
```

### 1.3.4 关闭 selinux

查看selinux是否开启：

```bash
$ getenforce
Enforcing		# 标识开启
```

临时关闭selinux，重启之后，无效：

```shell
$ setenforce 0

$ getenforce
Permissive
```

永久关闭selinux，需要重启：

```shell
$ sed -i 's/enforcing/disabled/' /etc/selinux/config
```


### 1.3.5 关闭交换区

删除 swap 区所有内容

```bash
$ swapoff -a
```

删除 swap 挂载，这样系统下次启动不会再挂载 swap

```bash
# 注释 swap 行
$ vim /etc/fstab

# 或者直接执行 shell 命令
$ sed -ri 's/.*swap.*/#&/' /etc/fstab
```

![image-20210519195200095](../img/image-20210519195200095.png)

```bash
# 测试
$ free -h
```

swap 一行应该全部是 0

![image-20210519195648371](../img/image-20210519195648371.png)

### 1.3.6 将桥接的 IPv4 流量传递到 iptables 的链

在每个节点上将桥接的IPv4流量传递到iptables的链：

```bash
$ cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
EOF
```

加载br_netfilter模块

``` bash
$ modprobe br_netfilter
```

查看是否加载

``` bash
$ lsmod | grep br_netfilter
br_netfilter           22256  0
bridge                151336  1 br_netfilter
```

生效

``` bash
$ sysctl --system
* Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
kernel.kptr_restrict = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
* Applying /etc/sysctl.conf ...
```

### 1.3.7 开启ipvs

在kubernetes中service有两种代理模型，一种是基于iptables，另一种是基于ipvs的。ipvs的性能要高于iptables的，但是如果要使用它，需要手动载入ipvs模块。

在每个节点安装ipset和ipvsadm：

```bash
$ yum -y install ipset ipvsadm
```

在所有节点执行如下脚本：

```bash
$ cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
```

授权、运行：

```bash
$ chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
ip_vs_sh               12688  0
ip_vs_wrr              12697  0
ip_vs_rr               12600  0
ip_vs                 145458  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr
nf_conntrack_ipv4      15053  9
nf_defrag_ipv4         12729  1 nf_conntrack_ipv4
nf_conntrack          139264  9 ip_vs,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_nat_masquerade_ipv6,nf_conntrack_ipv4,nf_conntrack_ipv6
libcrc32c              12644  4 xfs,ip_vs,nf_nat,nf_conntrack
```

检查是否加载：

```bash
$ lsmod | grep -e ipvs -e nf_conntrack_ipv4
nf_conntrack_ipv4      15053  10
nf_defrag_ipv4         12729  1 nf_conntrack_ipv4
nf_conntrack          139264  10 ip_vs,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_nat_masquerade_ipv6,nf_conntrack_netlink,nf_conntrack_ipv4,nf_conntrack_ipv6
```
### 1.3.8 重启机器

``` bash
$ reboot
```

# 2.安装 docker

帮助文档：[docker 官方帮助文档](https://docs.docker.com/)

## 2.1 需要的安装包

``` shell
$ yum install -y yum-utils
```

## 2.2 设置镜像的仓库

``` shell
# 阿里云镜像仓库，推荐使用
$ yum-config-manager \
    --add-repo \
    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
```

## 2.3 安装

安装 docker docker-ce 社区版 ee 企业版

``` shell
$ yum install docker-ce docker-ce-cli containerd.io
```

## 2.4 启动 docker

``` shell
# 设置 docker 开机自启动并启动 docker
$ systemctl enable docker
$ systemctl start docker
```

## 2.5 查看版本

使用 `docker version` 查看是否安装成功

![查看 docker 是否安装成功](../img/image-20210427003114853.png)

## 2.6 配置 Docker 守护程序

1.配置 Docker 守护程序，尤其是使用 systemd 来管理容器的 cgroup

```shell
$ mkdir /etc/docker
$ tee /etc/docker/daemon.json <<-'EOF'
{
  "exec-opts": ["native.cgroupdriver=systemd"], 
  "registry-mirrors": ["https://b9pmyelo.mirror.aliyuncs.com"], 
  "live-restore": true,
  "log-driver":"json-file",
  "log-opts": {"max-size":"500m", "max-file":"3"}
}
EOF
```

2.重新启动 Docker

```shell
$ systemctl daemon-reload
$ systemctl restart docker
```

# 3.安装 k8s

你需要在每台机器上安装以下的软件包：

- `kubeadm`：用来初始化集群的指令。
- `kubelet`：在集群中的每个节点上用来启动 Pod 和容器等。
- `kubectl`：用来与集群通信的命令行工具。

kubeadm **不能** 帮你安装或者管理 `kubelet` 或 `kubectl`，所以你需要确保它们与通过 kubeadm 安装的控制平面的版本相匹配。 如果不这样做，则存在发生版本偏差的风险，可能会导致一些预料之外的错误和问题。 然而，控制平面与 kubelet 间的相差一个次要版本不一致是支持的，但 kubelet 的版本不可以超过 API 服务器的版本。 例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。

## 3.1 安装过程

安装 kubeadm、kubelet 和 kubectl

### 3.1.1 添加阿里云的YUM软件源

由于kubernetes的镜像源在国外，非常慢，这里切换成国内的阿里云镜像源：

```bash
$ cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

### 3.1.2 安装kubeadm、kubelet和kubectl

由于版本更新频繁，这里指定版本号部署：

```bash
$ yum install -y kubelet-1.21.0 kubeadm-1.21.0 kubectl-1.21.0
```

为了实现 Docker 使用的 cgroup drvier 和 kubelet 使用的 cgroup drver 一致，建议修改 `/etc/sysconfig/kubelet` 文件的内容：

```bash
$ vim /etc/sysconfig/kubelet

# 修改
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"
KUBE_PROXY_MODE="ipvs"
```

设置为开机自启动即可，由于没有生成配置文件，集群初始化后自动启动：

```bash
$ systemctl enable kubelet
```

## 3.2 部署k8s的Master节点

**只有 master 节点才需要初始化集群**，在主机 10.252.74.100 上执行命令：`kubeadm init`

``` bash
$ kubeadm init \
  --apiserver-advertise-address=10.252.74.100 \
  --kubernetes-version v1.21.0 \
  --pod-network-cidr=10.244.0.0/16

# 最后出现下列信息表示初始化成功
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

# 在别的机器上执行下面的命令来加入集群
kubeadm join 10.252.74.100:6443 --token rpslvk.oll8evpz95qvnk7m \
         --discovery-token-ca-cert-hash sha256:88637abe412a5b07679afd049a534f0f2ae60e3e2fb16c75d438a88273cef346
```

要使非 root 用户可以运行 kubectl，请运行以下命令， 它们也是 `kubeadm init` 输出的一部分：

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

或者，如果你是 `root` 用户，则可以运行：

```bash
$ export KUBECONFIG=/etc/kubernetes/admin.conf
```

## 3.3 加入集群

在主机 10.252.74.49 上执行 `kubeadm join` 命令，出现下面输出表示加入成功

``` bash
$ kubeadm join 10.252.74.100:6443 --token rpslvk.oll8evpz95qvnk7m \
         --discovery-token-ca-cert-hash sha256:88637abe412a5b07679afd049a534f0f2ae60e3e2fb16c75d438a88273cef346

        [preflight] Running pre-flight checks
        [WARNING FileExisting-socat]: socat not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

默认的token有效期为24小时，当过期之后，该token就不能用了，这时可以使用如下的命令创建token：

``` shell
# 生成一个 token
$ kubeadm token create --print-join-command

# 生成一个永不过期的token
$ kubeadm token create --ttl 0 --print-join-command
```

可以通过 `kubectl get nodes` 查看集群中的节点

``` bash
$ kubectl get nodes
NAME         STATUS     ROLES                  AGE     VERSION
k8s-master   NotReady   control-plane,master   8m39s   v1.21.1
k8s-node-1   NotReady   <none>                 111s    v1.21.1
```

## 3.4 部署网络插件

使用 Weave Net 提供 NetworkPolicy

``` shell
# 下载插件
$ wget -O weave.yml https://cloud.weave.works/k8s/net?k8s-version=1.21.0

# 使用配置文件启动 flannel
$ kubectl apply -f weave.yml

# 查看部署  网络插件进度，直到所有 pod 状态为 Running 表示安装成功
$ kubectl get pods -n kube-system
NAME                                 READY   STATUS    RESTARTS   AGE
coredns-558bd4d5db-2vgdh             1/1     Running   0          3h28m
coredns-558bd4d5db-j5zpm             1/1     Running   0          3h28m
etcd-k8s-master                      1/1     Running   0          3h29m
kube-apiserver-k8s-master            1/1     Running   0          3h29m
kube-controller-manager-k8s-master   1/1     Running   0          3h18m
kube-proxy-dbmft                     1/1     Running   0          3h28m
kube-proxy-vsm7w                     1/1     Running   0          3h28m
kube-scheduler-k8s-master            1/1     Running   0          3h18m
weave-net-sghbj                      2/2     Running   1          3h23m
weave-net-x5xj7                      2/2     Running   1          3h23m
```

查看集群健康状况，命令：`kubectl get cs`

``` bash
$ kubectl get cs			# 发现报如下错误：
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS      MESSAGE                                                                                       ERROR
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused
controller-manager   Unhealthy   Get "http://127.0.0.1:10252/healthz": dial tcp 127.0.0.1:10252: connect: connection refused
etcd-0               Healthy     {"health":"true"}
```

``` bash
# 出现这种情况，是 /etc/kubernetes/manifests/ 下的 kube-controller-manager.yaml 和 kube-scheduler.yaml 设置的默认端口是0导致的
# 解决方式是注释掉对应的 port 即可，操作如下：

$ vim /etc/kubernetes/manifests/kube-controller-manager.yaml
```

![image-20210520213929379](../img/image-20210520213929379.png)

``` bash
$ vim /etc/kubernetes/manifests/kube-scheduler.yaml
```

![image-20210520214147092](../img/image-20210520214147092.png)

``` bash
# 重启 kubelet
$ systemctl restart kubelet

# 再次查看集群健康状况
$ kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health":"true"}

$ kubectl cluster-info
Kubernetes control plane is running at https://10.252.74.48:6443
CoreDNS is running at https://10.252.74.48:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

# 再次 get nodes 状态已经是 Ready
$ kubectl get nodes
NAME         STATUS   ROLES                  AGE     VERSION
k8s-master   Ready    control-plane,master   3h32m   v1.21.0
k8s-node-1   Ready    <none>                 3h31m   v1.21.0
```

# 4.部署 openfaas

## 4.1 安装 openfaas-cli

安装 openfaas-cli

``` bash
$ curl -sL https://cli.openfaas.com | sh
```

## 4.2 使用 k8s 的 yaml 文件安装 openfaas

### 4.2.1 创建 namespace

创建 namespace

``` bash
$ git clone https://github.com/openfaas/faas-netes
$ cd faas-netes
$ kubectl apply -f namespaces.yml
```

### 4.2.2 创建 admin 的密码

```bash
$ kubectl -n openfaas create secret generic basic-auth \
	--from-literal=basic-auth-user=admin \
	--from-literal=basic-auth-password=admin
```

### 4.2.3 部署openfaas

部署 openfaas

```bash
$ kubectl apply -f ./yaml
```

查看部署的状态，直到所有组件的状态都是 **Running**

``` bash
$ kubectl get pods -n openfaas
NAME                                 READY   STATUS    RESTARTS   AGE
alertmanager-86f8447b8d-92g5f        1/1     Running   0          140m
basic-auth-plugin-78c65cd966-dxc4q   1/1     Running   0          140m
gateway-f79d87f45-6tbpz              2/2     Running   0          140m
nats-6b6564d858-x97gx                1/1     Running   0          140m
prometheus-59fd74885f-sl8z4          1/1     Running   0          140m
queue-worker-5f6cb648db-mfjx5        1/1     Running   0          140m
```

### 4.2.4 设置环境变量

设置环境变量OPENFAAS_URL

```bash
$ export OPENFAAS_URL=http://127.0.0.1:31112
```

### 4.2.5 登录 openfaas

```bash
$ faas-cli login -u admin --password admin
```

### 4.2.6 使用浏览器访问 openfaas

 使用浏览器访问 openfaas 的 UI：http://10.252.74.100:31112/

# 5.部署 DashBoard

## 5.1 安装 DashBoard

1.下载 yaml

``` bash
$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml
```

2.修改 yaml，service段配置加上 `type: NodePort`

``` bash
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  # 加上这一行，原本是没有的
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard
```

3.启动服务

``` bash
$ kubectl apply -f recommended.yaml
```

4.查看 services，发现对外暴露端口为：32539

``` bash
$ kubectl get svc -n kubernetes-dashboard
NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
dashboard-metrics-scraper   ClusterIP   10.103.203.250   <none>        8000/TCP        29m
kubernetes-dashboard        NodePort    10.104.110.211   <none>        443:32539/TCP   29m
```

5.浏览器访问：https://10.252.74.100:32539，会看到如下页面，接下来要创建账户

![image-20210526151703977](../img/image-20210526151703977.png)

## 5.2 创建账户

1.创建服务账户（serviceaccount）资源

``` bash
$ kubectl create serviceaccount ui-admin -n kube-system

# 查看服务账户
$ kubectl get sa -n kube-system ui-admin
NAME       SECRETS   AGE
ui-admin   1         81m
```

2.创建集群角色绑定

``` bash
# 查看集群角色
$ kubectl get clusterrole -n kube-system cluster-admin
NAME            CREATED AT
cluster-admin   2021-05-25T06:32:03Z

# 创建集群角色绑定
$ kubectl create clusterrolebinding cluster-ui-admin --clusterrole=cluster-admin --serviceaccount=kube-system:ui-admin

# 查看集群角色绑定
$ kubectl get clusterrolebinding cluster-ui-admin
NAME               ROLE                        AGE
cluster-ui-admin   ClusterRole/cluster-admin   83m
```

3.获取 ui-admin 的 token

``` bash
# 以 yaml 的方式打印 ui-admin 服务账户的详细信息
$ kubectl get sa ui-admin -n kube-system -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: "2021-05-26T05:45:05Z"
  name: ui-admin
  namespace: kube-system
  resourceVersion: "107957"
  uid: f7b29183-522d-4876-8dfe-a3df845dc320
secrets:
- name: ui-admin-token-8rkkw

# 生成 token
$ kubectl describe secret ui-admin-token-8rkkw -n kube-system
Name:         ui-admin-token-8rkkw
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: ui-admin
              kubernetes.io/service-account.uid: f7b29183-522d-4876-8dfe-a3df845dc320

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1066 bytes
namespace:  11 bytes
# 这个就是 token
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IlNZMkhibWo0RE1EOEd3eFA5QlB6a1BJWUN3LTJKZWdKQUVQOGVYV216N2sifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ1aS1hZG1pbi10b2tlbi04cmtrdyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ1aS1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImY3YjI5MTgzLTUyMmQtNDg3Ni04ZGZlLWEzZGY4NDVkYzMyMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTp1aS1hZG1pbiJ9.Mzd1Ma0XvnFj-wy2WPyT7AwiGTBnABUu_ZYZsuySx_owRCMIerw5QeOyBqEpF0DDrqB_HuBQyA9P9SIhdDICNI8IXkJ9yFWcnpIGogj3tp7gbNKvRxAOiErmNcojEw23-kLIr5phK9RLuU_Irp49NJDx42uHXhAs9HgSlf1cVdwUNSr45ZMuDeDauT7rul8ROSkuu5wnHljWvm8nbE9Q4glN4D0tOWUr39IY9DhJJdIrSGxS8-rUUELrlo6_0DoWO_pccDgLs44aeR76HEKK7PKA-xUQfv1g2g25xpoe7yGLURWK8lLmqo9Qh9YZ9GrgGdkYwLfwIf4y4Qyfe-PPTQ
```

因为用 token 登录会出现下面的问题，所以不用这种方式登录，用下面生成 Kubeconfig 的方式登录

![image-20210526153023300](../img/image-20210526153023300.png)

4.创建 kubeconfig 配置文件

``` bash
# 第一步
$ kubectl config set-cluster k8s-master --server="https://10.74.252.100:6443" --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --kubeconfig=/tmp/ui-admin.kubeconfig

# 第二步，这里的 token 用上面生成的
$ kubectl config set-credentials ui-admin --token=eyJhbGciOiJSUzI1NiIsImtpZCI6IlNZMkhibWo0RE1EOEd3eFA5QlB6a1BJWUN3LTJKZWdKQUVQOGVYV216N2sifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ1aS1hZG1pbi10b2tlbi04cmtrdyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ1aS1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImY3YjI5MTgzLTUyMmQtNDg3Ni04ZGZlLWEzZGY4NDVkYzMyMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTp1aS1hZG1pbiJ9.Mzd1Ma0XvnFj-wy2WPyT7AwiGTBnABUu_ZYZsuySx_owRCMIerw5QeOyBqEpF0DDrqB_HuBQyA9P9SIhdDICNI8IXkJ9yFWcnpIGogj3tp7gbNKvRxAOiErmNcojEw23-kLIr5phK9RLuU_Irp49NJDx42uHXhAs9HgSlf1cVdwUNSr45ZMuDeDauT7rul8ROSkuu5wnHljWvm8nbE9Q4glN4D0tOWUr39IY9DhJJdIrSGxS8-rUUELrlo6_0DoWO_pccDgLs44aeR76HEKK7PKA-xUQfv1g2g25xpoe7yGLURWK8lLmqo9Qh9YZ9GrgGdkYwLfwIf4y4Qyfe-PPTQ --kubeconfig=/tmp/ui-admin.kubeconfig

# 第三步
$ kubectl config set-context ui-admin@k8s-master --cluster=k8s-master --user=ui-admin --kubeconfig=/tmp/ui-admin.kubeconfig

# 第四步
$ kubectl config use-context ui-admin@k8s-master --kubeconfig=/tmp/ui-admin.kubeconfig

# 第五步，输出 kubeconfig 配置
$ kubectl config view --kubeconfig=/tmp/ui-admin.kubeconfig
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://10.74.252.100:6443
  name: k8s-master
contexts:
- context:
    cluster: k8s-master
    user: ui-admin
  name: ui-admin@k8s-master
current-context: ui-admin@k8s-master
kind: Config
preferences: {}
users:
- name: ui-admin
  user:
    # 这里的 token 换成上面生成的 token 即可，然后保存成一个 yaml 文件
    token: REDACTED
```

完整的 token.yaml 文件：

``` yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://10.74.252.100:6443
  name: k8s-master
contexts:
- context:
    cluster: k8s-master
    user: ui-admin
  name: ui-admin@k8s-master
current-context: ui-admin@k8s-master
kind: Config
preferences: {}
users:
- name: ui-admin
  user:
    token: eyJhbGciOiJSUzI1NiIsImtpZCI6IlNZMkhibWo0RE1EOEd3eFA5QlB6a1BJWUN3LTJKZWdKQUVQOGVYV216N2sifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ1aS1hZG1pbi10b2tlbi04cmtrdyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ1aS1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImY3YjI5MTgzLTUyMmQtNDg3Ni04ZGZlLWEzZGY4NDVkYzMyMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTp1aS1hZG1pbiJ9.Mzd1Ma0XvnFj-wy2WPyT7AwiGTBnABUu_ZYZsuySx_owRCMIerw5QeOyBqEpF0DDrqB_HuBQyA9P9SIhdDICNI8IXkJ9yFWcnpIGogj3tp7gbNKvRxAOiErmNcojEw23-kLIr5phK9RLuU_Irp49NJDx42uHXhAs9HgSlf1cVdwUNSr45ZMuDeDauT7rul8ROSkuu5wnHljWvm8nbE9Q4glN4D0tOWUr39IY9DhJJdIrSGxS8-rUUELrlo6_0DoWO_pccDgLs44aeR76HEKK7PKA-xUQfv1g2g25xpoe7yGLURWK8lLmqo9Qh9YZ9GrgGdkYwLfwIf4y4Qyfe-PPTQ
```

## 5.3 登录 Dashboard

选择 Kubeconfig 登录方式，然后上传 token.yaml 文件，点登录就好了

![image-20210526153914564](../img/image-20210526153914564.png)

登录后的界面是这样的：

![image-20210526154009891](../img/image-20210526154009891.png)

