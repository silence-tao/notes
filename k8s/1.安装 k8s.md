# 1.环境准备

## 1.1 主机规划

> 一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令
>
> 每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)
>
> 2 CPU 核或更多
>
> 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)

| 角色   | IP 地址      | 操作系统                             | 配置                        |
| ------ | ------------ | ------------------------------------ | --------------------------- |
| master | 10.252.74.48 | CentOS Linux release 7.9.2009 (Core) | 2 核 CPU，2G 内存，50G 硬盘 |
| node-1 | 10.252.74.49 | CentOS Linux release 7.9.2009 (Core) | 2 核 CPU，2G 内存，50G 硬盘 |

## 1.2 环境查看

通过命令 `uname -r` 查看系统内核环境，系统环境必须是 3.10 以上的

``` shell
[root@VM_0_9_centos /]# uname -r
3.10.0-862.el7.x86_64
```
通过命令 `cat /etc/os-release` 查看系统属性
``` shell
[root@VM_0_9_centos /]# cat /etc/os-release 
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"
```

通过命令 `cat /etc/redhat-release` 查看系统版本（要求操作系统的版本至少在7.5以上）

``` bash
[root@localhost ~]# cat /etc/redhat-release
CentOS Linux release 7.9.2009 (Core)
```
## 1.3 环境搭建

### 1.3.1 主机名解析

为了方便后面集群节点间的直接调用，需要配置一下主机名解析，企业中推荐使用内部的DNS服务器。

```shell
cat >> /etc/hosts << EOF
10.252.74.48 master
10.252.74.49 node-1

# github 的一些域名的 DNS 解析被污染，会导致 DNS 解析过程无法通过域名取得正确的IP地址
# 所以这里添加域名的绑定
199.232.68.133 raw.githubusercontent.com
199.232.68.133 user-images.githubusercontent.com
199.232.68.133 avatars2.githubusercontent.com
199.232.68.133 avatars1.githubusercontent.com
EOF
```

### 1.3.2 时间同步

kubernetes要求集群中的节点时间必须精确一致，所以在每个节点上添加时间同步：

```shell
yum install ntpdate -y

# 测试时间同步
ntpdate time.windows.com
```

### 1.3.3 关闭 selinux

查看selinux是否开启：

```bash
[root@localhost ~]# getenforce
Enforcing		# 标识开启
```

临时关闭selinux，重启之后，无效：

```shell
[root@k8s-master ~]# setenforce 0
[root@k8s-master ~]# getenforce
Permissive
```

永久关闭selinux，需要重启：

```shell
sed -i 's/enforcing/disabled/' /etc/selinux/config
```


### 1.3.4 关闭交换区

删除 swap 区所有内容

```bash
swapoff -a
```

删除 swap 挂载，这样系统下次启动不会再挂载 swap

```bash
# 注释 swap 行
vim /etc/fstab
```

![image-20210519195200095](../img/image-20210519195200095.png)

```cpp
# 测试
free -h
```

swap 一行应该全部是 0

![image-20210519195648371](../img/image-20210519195648371.png)

### 1.3.5 安装 conntrack

``` shell
# 命令
yum install conntrack
```

### 1.3.6 关闭防火墙

``` shell
# 查看防火墙状态
firewall-cmd --state

# 关闭防火墙
[root@localhost .kube]# systemctl stop firewalld.service

# 禁止防火墙开机自启动
[root@localhost .kube]# systemctl disable firewalld.service
Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.
Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.
```

### 1.3.7 将桥接的 IPv4 流量传递到 iptables 的链

在每个节点上将桥接的IPv4流量传递到iptables的链：

```shell
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
EOF
```

```shell
# 加载br_netfilter模块
modprobe br_netfilter
```

```shell
# 查看是否加载
[root@localhost ~]# lsmod | grep br_netfilter
br_netfilter           22256  0
bridge                151336  1 br_netfilter
```

```shell
# 生效
[root@localhost ~]# sysctl --system
* Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
kernel.kptr_restrict = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
* Applying /etc/sysctl.conf ...
```

### 1.3.8 开启ipvs

在kubernetes中service有两种代理模型，一种是基于iptables，另一种是基于ipvs的。ipvs的性能要高于iptables的，但是如果要使用它，需要手动载入ipvs模块。

在每个节点安装ipset和ipvsadm：

```bash
yum -y install ipset ipvsadm
```

在所有节点执行如下脚本：

```bash
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
```

授权、运行：

```bash
[root@localhost ~]# chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
ip_vs_sh               12688  0
ip_vs_wrr              12697  0
ip_vs_rr               12600  0
ip_vs                 145458  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr
nf_conntrack_ipv4      15053  9
nf_defrag_ipv4         12729  1 nf_conntrack_ipv4
nf_conntrack          139264  9 ip_vs,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_nat_masquerade_ipv6,nf_conntrack_ipv4,nf_conntrack_ipv6
libcrc32c              12644  4 xfs,ip_vs,nf_nat,nf_conntrack
```

检查是否加载：

```bash
[root@localhost ~]# lsmod | grep -e ipvs -e nf_conntrack_ipv4
nf_conntrack_ipv4      15053  10
nf_defrag_ipv4         12729  1 nf_conntrack_ipv4
nf_conntrack          139264  10 ip_vs,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_nat_masquerade_ipv6,nf_conntrack_netlink,nf_conntrack_ipv4,nf_conntrack_ipv6
```
# 2.安装 docker

帮助文档：[docker 官方帮助文档](https://docs.docker.com/)

## 2.1 卸载旧的版本

卸载旧的版本，如果有的话

``` shell
sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
```

## 2.2 需要的安装包

``` shell
sudo yum install -y yum-utils
```

## 2.3 设置镜像的仓库

``` shell
# 默认是境外的仓库，很慢不建议使用
sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
    
# 阿里云镜像仓库，推荐使用
sudo yum-config-manager \
    --add-repo \
    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
    
# 更新软件包索引
yum makecache fast
```

## 2.4 安装

安装 docker docker-ce 社区版 ee 企业版

``` shell
sudo yum install docker-ce docker-ce-cli containerd.io
```

## 2.5 启动 docker

``` shell
# 设置 docker 开机自启动并启动 docker
sudo systemctl enable docker
sudo systemctl start docker
```

## 2.6 查看版本

使用 `docker version` 查看是否安装成功

![查看 docker 是否安装成功](../img/image-20210427003114853.png)

## 2.7 配置 Docker 守护程序

1.配置 Docker 守护程序，尤其是使用 systemd 来管理容器的 cgroup

```shell
sudo mkdir /etc/docker
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
```

2.重新启动 Docker

```shell
sudo systemctl daemon-reload
sudo systemctl restart docker
```

# 3.安装 k8s

你需要在每台机器上安装以下的软件包：

- `kubeadm`：用来初始化集群的指令。
- `kubelet`：在集群中的每个节点上用来启动 Pod 和容器等。
- `kubectl`：用来与集群通信的命令行工具。

kubeadm **不能** 帮你安装或者管理 `kubelet` 或 `kubectl`，所以你需要确保它们与通过 kubeadm 安装的控制平面的版本相匹配。 如果不这样做，则存在发生版本偏差的风险，可能会导致一些预料之外的错误和问题。 然而，控制平面与 kubelet 间的相差一个次要版本不一致是支持的，但 kubelet 的版本不可以超过 API 服务器的版本。 例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。

## 3.1 安装过程

安装 kubeadm、kubelet 和 kubectl

### 3.1.1 安装 CNI 插件

安装 CNI 插件（大多数 Pod 网络都需要）：

```bash
CNI_VERSION="v0.8.2"
sudo mkdir -p /opt/cni/bin
curl -L "https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-linux-amd64-${CNI_VERSION}.tgz" | sudo tar -C /opt/cni/bin -xz
```

### 3.1.2 定义要下载的目录

```bash
DOWNLOAD_DIR=/usr/local/bin
sudo mkdir -p $DOWNLOAD_DIR
```

### 3.1.3 安装 crictl

 crictl（kubeadm/kubelet 容器运行时接口（CRI）所需）

```bash
CRICTL_VERSION="v1.17.0"
curl -L "https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz" | sudo tar -C $DOWNLOAD_DIR -xz
```

### 3.1.4 下载

下载 `kubeadm`、`kubelet`、`kubectl` 

``` bash
# 下载的版本号
RELEASE="v1.21.1"
# 进入下载目录
cd $DOWNLOAD_DIR

# 下载 kubeadm, kubelet, kubectl
sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/${RELEASE}/bin/linux/amd64/{kubeadm,kubelet,kubectl}

# 添加权限
sudo chmod +x {kubeadm,kubelet,kubectl}
```

### 3.1.5 获取配置文件

```bash
# 配置文件版本号
RELEASE_VERSION="v0.4.0"

# 获取 kubelet.service
curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service

# 获取 10-kubeadm.conf
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
```

### 3.1.6 配置 cgroup

为了实现Docker使用的cgroup drvier和kubelet使用的cgroup drver一致，建议修改"/etc/sysconfig/kubelet"文件的内容：

``` shell
vim /etc/sysconfig/kubelet

# 修改
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"
KUBE_PROXY_MODE="ipvs"
```

### 3.1.7 激活并启动

激活并启动 `kubelet`：

```bash
systemctl enable --now kubelet
```

## 3.2 初始化集群

**只有 master 节点才需要初始化集群**，在主机 10.252.74.48 上执行命令：`kubeadm init`

``` bash
[root@localhost ~]# kubeadm init --apiserver-advertise-address=10.252.74.48 --pod-network-cidr=10.244.0.0/16
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

# 在别的机器上执行下面的命令来加入集群
kubeadm join 10.252.74.48:6443 --token 431zi7.xriz7iws4rvt34gk \
        --discovery-token-ca-cert-hash sha256:e0aeb2c222963e366a1323e21f3dc327b3474b799b51d0fab8089ca226384d84
```

要使非 root 用户可以运行 kubectl，请运行以下命令， 它们也是 `kubeadm init` 输出的一部分：

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

或者，如果你是 `root` 用户，则可以运行：

```bash
export KUBECONFIG=/etc/kubernetes/admin.conf
```

## 3.3 加入集群

在主机 10.252.74.49 上执行 `kubeadm join` 命令，出现下面输出表示加入成功

``` bash
[root@localhost ~]# kubeadm join 10.252.74.48:6443 --token 431zi7.xriz7iws4rvt34gk \
        --discovery-token-ca-cert-hash sha256:e0aeb2c222963e366a1323e21f3dc327b3474b799b51d0fab8089ca226384d84
        [preflight] Running pre-flight checks
        [WARNING FileExisting-socat]: socat not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

默认的token有效期为24小时，当过期之后，该token就不能用了，这时可以使用如下的命令创建token：

``` shell
# 生成一个 token
kubeadm token create --print-join-command

# 生成一个永不过期的token
kubeadm token create --ttl 0 --print-join-command
```

可以通过 `kubectl get nodes` 查看集群中的节点

``` bash
[root@k8s-master ~]# kubectl get nodes
NAME         STATUS     ROLES                  AGE     VERSION
k8s-master   NotReady   control-plane,master   8m39s   v1.21.1
k8s-node-1   NotReady   <none>                 111s    v1.21.1
```

## 3.4 部署 CNI 网络插件

kubernetes 支持多种网络插件，比如 flannel、calico、canal 等，任选一种即可，本次选择 flannel

在 Master 节点上获取 flannel 配置文件(可能会失败，如果失败，请下载到本地，然后安装)：

``` shell
# 下载插件
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

# 使用配置文件启动 flannel
kubectl apply -f kube-flannel.yml

# 查看部署 CNI 网络插件进度
[root@k8s-master k8s]# kubectl get pods -n kube-system
NAME                                 READY   STATUS     RESTARTS   AGE
coredns-558bd4d5db-5b8r6             0/1     Running    0          4m11s
coredns-558bd4d5db-t5hjs             1/1     Running    0          4m11s
etcd-k8s-master                      1/1     Running    1          4m23s
kube-apiserver-k8s-master            1/1     Running    1          4m23s
kube-controller-manager-k8s-master   1/1     Running    2          4m23s
kube-flannel-ds-828vt                1/1     Running    0          23s
kube-flannel-ds-phvkl                0/1     Init:0/1   0          23s
kube-proxy-czlm5                     1/1     Running    0          110s
kube-proxy-z6xxn                     1/1     Running    1          4m11s
kube-scheduler-k8s-master            1/1     Running    1          4m26s
```

查看集群健康状况，命令：`kubectl get cs`

``` bash
[root@k8s-master k8s]# kubectl get cs			# 发现报如下错误：
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS      MESSAGE                                                                                       ERROR
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused
controller-manager   Unhealthy   Get "http://127.0.0.1:10252/healthz": dial tcp 127.0.0.1:10252: connect: connection refused
etcd-0               Healthy     {"health":"true"}
```

``` bash
# 出现这种情况，是 /etc/kubernetes/manifests/ 下的 kube-controller-manager.yaml 和 kube-scheduler.yaml 设置的默认端口是0导致的
# 解决方式是注释掉对应的 port 即可，操作如下：

vi /etc/kubernetes/manifests/kube-controller-manager.yaml
```

![image-20210520213929379](../img/image-20210520213929379.png)

``` bash
vi /etc/kubernetes/manifests/kube-scheduler.yaml
```

![image-20210520214147092](../img/image-20210520214147092.png)

``` bash
# 重启 kubelet
systemctl restart kubelet.service

# 再次查看集群健康状况
[root@k8s-master k8s]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health":"true"}

[root@k8s-master k8s]# kubectl cluster-info
Kubernetes control plane is running at https://10.252.74.48:6443
CoreDNS is running at https://10.252.74.48:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

# 4.测试服务部署

在 k8s 集群中部署一个 Nginx 程序，测试集群是否工作正常

``` bash
# 部署Nginx
[root@k8s-master k8s]# kubectl create deployment nginx --image=nginx:1.14-alpine
deployment.apps/nginx created

# 暴露端口
[root@k8s-master k8s]# kubectl expose deployment nginx --port=80 --type=NodePort
service/nginx exposed

# 查看服务状态
[root@k8s-master k8s]# kubectl get pods,svc
NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-65c4bffcb6-2ndxq   1/1     Running   0          23s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP        18m
service/nginx        NodePort    10.109.210.100   <none>        80:31941/TCP   7s
```



# 4.部署 openfaas

``` bash
[root@localhost ~]# mkdir github
[root@localhost ~]# ls
anaconda-ks.cfg  github
[root@localhost ~]# cd github/
[root@localhost github]# ls
[root@localhost github]# git clone https://github.com/openfaas/faas-netes
正克隆到 'faas-netes'...
remote: Enumerating objects: 17429, done.
remote: Counting objects: 100% (226/226), done.
remote: Compressing objects: 100% (171/171), done.
remote: Total 17429 (delta 132), reused 108 (delta 55), pack-reused 17203
接收对象中: 100% (17429/17429), 16.13 MiB | 1.49 MiB/s, done.
处理 delta 中: 100% (11083/11083), done.
[root@localhost github]# ls
faas-netes
[root@localhost github]# cd faas-netes/
[root@localhost faas-netes]# ls
artifacts  cloud    CONTRIBUTING.md  docs    go.sum  install.sh  main.go   namespaces.yml  pkg        README-OPERATOR.md  version
chart      contrib  Dockerfile       go.mod  hack    LICENSE     Makefile  openshift       README.md  vendor              yaml
[root@localhost faas-netes]# clear
[root@localhost faas-netes]# ls
artifacts  cloud    CONTRIBUTING.md  docs    go.sum  install.sh  main.go   namespaces.yml  pkg        README-OPERATOR.md  version
chart      contrib  Dockerfile       go.mod  hack    LICENSE     Makefile  openshift       README.md  vendor              yaml
[root@localhost faas-netes]# kubectl apply -f namespaces.yml
namespace/openfaas created
namespace/openfaas-fn created

[root@localhost faas-netes]# kubectl -n openfaas create secret generic basic-auth \
>     --from-literal=basic-auth-user=admin \
>     --from-literal=basic-auth-password=admin
secret/basic-auth created
[root@localhost faas-netes]# kubectl apply -f ./yaml/
configmap/alertmanager-config created
deployment.apps/alertmanager created
service/alertmanager created
deployment.apps/basic-auth-plugin created
service/basic-auth-plugin created
serviceaccount/openfaas-controller created
role.rbac.authorization.k8s.io/openfaas-controller created
role.rbac.authorization.k8s.io/openfaas-profiles created
rolebinding.rbac.authorization.k8s.io/openfaas-controller created
rolebinding.rbac.authorization.k8s.io/openfaas-profiles created
Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition
customresourcedefinition.apiextensions.k8s.io/profiles.openfaas.com created
deployment.apps/gateway created
service/gateway-external created
service/gateway created
deployment.apps/nats created
service/nats created
customresourcedefinition.apiextensions.k8s.io/profiles.openfaas.com configured
configmap/prometheus-config created
deployment.apps/prometheus created
serviceaccount/openfaas-prometheus created
role.rbac.authorization.k8s.io/openfaas-prometheus created
role.rbac.authorization.k8s.io/openfaas-prometheus-fn created
rolebinding.rbac.authorization.k8s.io/openfaas-prometheus created
rolebinding.rbac.authorization.k8s.io/openfaas-prometheus-fn created
service/prometheus created
deployment.apps/queue-worker created

[root@localhost faas-netes]# kubectl get pods -n openfaas
NAME                                 READY   STATUS    RESTARTS   AGE
alertmanager-86f8447b8d-x7pzn        0/1     Pending   0          64s
basic-auth-plugin-78c65cd966-wbhr8   0/1     Pending   0          64s
gateway-f79d87f45-blfm2              0/2     Pending   0          63s
nats-6b6564d858-qhgzm                0/1     Pending   0          62s
prometheus-59fd74885f-84t8p          0/1     Pending   0          62s
queue-worker-5f6cb648db-lq7f5        0/1     Pending   0          60s
```



