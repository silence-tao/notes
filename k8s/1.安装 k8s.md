# 1.环境准备

## 1.1 主机规划

> 一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令
>
> 每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)
>
> 2 CPU 核或更多
>
> 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)

| 角色   | IP 地址       | 操作系统                             | 配置                        |
| ------ | ------------- | ------------------------------------ | --------------------------- |
| master | 10.252.74.100 | CentOS Linux release 7.9.2009 (Core) | 2 核 CPU，4G 内存，50G 硬盘 |
| node-1 | 10.252.74.101 | CentOS Linux release 7.9.2009 (Core) | 2 核 CPU，4G 内存，50G 硬盘 |

## 1.2 环境查看

通过命令 `uname -r` 查看系统内核环境，系统环境必须是 3.10 以上的

``` shell
$ uname -r
3.10.0-862.el7.x86_64
```
通过命令 `cat /etc/os-release` 查看系统属性
``` shell
$ cat /etc/os-release 
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"
```

通过命令 `cat /etc/redhat-release` 查看系统版本（要求操作系统的版本至少在7.5以上）

``` bash
$ cat /etc/redhat-release
CentOS Linux release 7.9.2009 (Core)
```
## 1.3 环境搭建

### 1.3.1 关闭防火墙

``` shell
# 查看防火墙状态
$ firewall-cmd --state

# 关闭防火墙
$ systemctl stop firewalld

# 禁止防火墙开机自启动
$ systemctl disable firewalld
Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.
Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.
```

### 1.3.2 主机名解析

为了方便后面集群节点间的直接调用，需要配置一下主机名解析，企业中推荐使用内部的DNS服务器。

```shell
$ cat >> /etc/hosts << EOF
10.252.74.48 master
10.252.74.49 node-1

# github 的一些域名的 DNS 解析被污染，会导致 DNS 解析过程无法通过域名取得正确的IP地址
# 所以这里添加域名的绑定
199.232.68.133 raw.githubusercontent.com
199.232.68.133 user-images.githubusercontent.com
199.232.68.133 avatars2.githubusercontent.com
199.232.68.133 avatars1.githubusercontent.com
EOF
```

### 1.3.3 时间同步

kubernetes要求集群中的节点时间必须精确一致，所以在每个节点上添加时间同步：

```shell
# 启动 chronyd 服务
$ systemctl start chronyd

# 设置 chronyd 开机自启
$ systemctl enable chronyd
```

### 1.3.4 关闭 selinux

查看selinux是否开启：

```bash
$ getenforce
Enforcing		# 标识开启
```

临时关闭selinux，重启之后，无效：

```shell
$ setenforce 0

$ getenforce
Permissive
```

永久关闭selinux，需要重启：

```shell
$ sed -i 's/enforcing/disabled/' /etc/selinux/config
```


### 1.3.5 关闭交换区

删除 swap 区所有内容

```bash
$ swapoff -a
```

删除 swap 挂载，这样系统下次启动不会再挂载 swap

```bash
# 注释 swap 行
$ vim /etc/fstab

# 或者直接执行 shell 命令
$ sed -ri 's/.*swap.*/#&/' /etc/fstab
```

![image-20210519195200095](../img/image-20210519195200095.png)

```bash
# 测试
$ free -h
```

swap 一行应该全部是 0

![image-20210519195648371](../img/image-20210519195648371.png)

### 1.3.6 将桥接的 IPv4 流量传递到 iptables 的链

在每个节点上将桥接的IPv4流量传递到iptables的链：

```bash
$ cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
EOF
```

重新加载配置

``` bash
$ sysctl -p
```

加载网桥过滤模块

``` bash
$ modprobe br_netfilter
```

查看是否加载

``` bash
$ lsmod | grep br_netfilter
br_netfilter           22256  0
bridge                151336  1 br_netfilter
```

生效

``` bash
$ sysctl --system
* Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
kernel.kptr_restrict = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
* Applying /etc/sysctl.conf ...
```

### 1.3.7 开启ipvs

在kubernetes中service有两种代理模型，一种是基于iptables，另一种是基于ipvs的。ipvs的性能要高于iptables的，但是如果要使用它，需要手动载入ipvs模块。

在每个节点安装ipset和ipvsadm：

```bash
$ yum -y install ipset ipvsadm
```

在所有节点执行如下脚本：

```bash
$ cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
```

授权、运行：

```bash
$ chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
ip_vs_sh               12688  0
ip_vs_wrr              12697  0
ip_vs_rr               12600  0
ip_vs                 145458  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr
nf_conntrack_ipv4      15053  9
nf_defrag_ipv4         12729  1 nf_conntrack_ipv4
nf_conntrack          139264  9 ip_vs,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_nat_masquerade_ipv6,nf_conntrack_ipv4,nf_conntrack_ipv6
libcrc32c              12644  4 xfs,ip_vs,nf_nat,nf_conntrack
```

检查是否加载：

```bash
$ lsmod | grep -e ipvs -e nf_conntrack_ipv4
nf_conntrack_ipv4      15053  10
nf_defrag_ipv4         12729  1 nf_conntrack_ipv4
nf_conntrack          139264  10 ip_vs,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_nat_masquerade_ipv6,nf_conntrack_netlink,nf_conntrack_ipv4,nf_conntrack_ipv6
```
### 1.3.8 重启机器

``` bash
$ reboot
```

# 2.安装 docker

帮助文档：[docker 官方帮助文档](https://docs.docker.com/)

## 2.1 需要的安装包

``` shell
$ yum install -y yum-utils
```

## 2.2 设置镜像的仓库

``` shell
# 阿里云镜像仓库，推荐使用
$ yum-config-manager \
    --add-repo \
    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
```

## 2.3 安装

安装 docker docker-ce 社区版 ee 企业版

``` shell
$ yum install -y docker-ce docker-ce-cli containerd.io
```

## 2.4 启动 docker

``` shell
# 设置 docker 开机自启动并启动 docker
$ systemctl enable docker
$ systemctl start docker
```

## 2.5 查看版本

使用 `docker version` 查看是否安装成功

![查看 docker 是否安装成功](../img/image-20210427003114853.png)

## 2.6 配置 Docker 守护程序

1.配置 Docker 守护程序，尤其是使用 systemd 来管理容器的 cgroup

```shell
$ mkdir /etc/docker
$ tee /etc/docker/daemon.json <<-'EOF'
{
  "exec-opts": ["native.cgroupdriver=systemd"], 
  "registry-mirrors": ["https://b9pmyelo.mirror.aliyuncs.com"], 
  "live-restore": true,
  "log-driver":"json-file",
  "log-opts": {"max-size":"500m", "max-file":"3"}
}
EOF
```

2.重新启动 Docker

```shell
$ systemctl daemon-reload
$ systemctl restart docker
```

# 3.安装 k8s

你需要在每台机器上安装以下的软件包：

- `kubeadm`：用来初始化集群的指令。
- `kubelet`：在集群中的每个节点上用来启动 Pod 和容器等。
- `kubectl`：用来与集群通信的命令行工具。

kubeadm **不能** 帮你安装或者管理 `kubelet` 或 `kubectl`，所以你需要确保它们与通过 kubeadm 安装的控制平面的版本相匹配。 如果不这样做，则存在发生版本偏差的风险，可能会导致一些预料之外的错误和问题。 然而，控制平面与 kubelet 间的相差一个次要版本不一致是支持的，但 kubelet 的版本不可以超过 API 服务器的版本。 例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。

## 3.1 安装过程

安装 kubeadm、kubelet 和 kubectl

### 3.1.1 添加阿里云的YUM软件源

由于kubernetes的镜像源在国外，非常慢，这里切换成国内的阿里云镜像源：

```bash
$ cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

### 3.1.2 安装kubeadm、kubelet和kubectl

由于版本更新频繁，这里指定版本号部署：

```bash
$ yum install -y kubelet-1.21.0 kubeadm-1.21.0 kubectl-1.21.0
```

为了实现 Docker 使用的 cgroup drvier 和 kubelet 使用的 cgroup drver 一致，建议修改 `/etc/sysconfig/kubelet` 文件的内容：

```bash
$ vim /etc/sysconfig/kubelet

# 修改
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"
KUBE_PROXY_MODE="ipvs"
```

设置为开机自启动即可，由于没有生成配置文件，集群初始化后自动启动：

```bash
$ systemctl enable kubelet
```

## 3.2 部署k8s的Master节点

在安装 k8s 集群之前，必须要提前准备好集群需要的镜像，所需镜像可以通过下面的命令查看

``` bash
$ kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.21.1
k8s.gcr.io/kube-controller-manager:v1.21.1
k8s.gcr.io/kube-scheduler:v1.21.1
k8s.gcr.io/kube-proxy:v1.21.1
k8s.gcr.io/pause:3.4.1
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns/coredns:v1.8.0
```

每一台主机都要下载这些镜像，可以通过下面的 shell 脚本提前下载好 k8s 集群需要的镜像：

``` shell
#!/bin/bash

images=(
	kube-apiserver:v1.21.0
	kube-controller-manager:v1.21.0
	kube-scheduler:v1.21.0
	kube-proxy:v1.21.0
	pause:3.4.1
	etcd:3.4.13-0
	coredns:1.8.0
)


for imageName in ${images[@]} ; do
	docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
	docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
	docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done

# 注意：coredns 要改成 k8s.gcr.io/coredns/coredns:v1.8.0
docker tag k8s.gcr.io/coredns:1.8.0 k8s.gcr.io/coredns/coredns:v1.8.0
docker rmi k8s.gcr.io/coredns:1.8.0
```

**只有 master 节点才需要初始化集群**，在主机 10.252.74.100 上执行命令：`kubeadm init`

``` bash
$ kubeadm init \
  --apiserver-advertise-address=10.252.74.100 \
  --kubernetes-version v1.21.0 \
  --pod-network-cidr=10.244.0.0/16

# 最后出现下列信息表示初始化成功
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

# 在别的机器上执行下面的命令来加入集群
kubeadm join 10.252.74.100:6443 --token rpslvk.oll8evpz95qvnk7m \
         --discovery-token-ca-cert-hash sha256:88637abe412a5b07679afd049a534f0f2ae60e3e2fb16c75d438a88273cef346
```

要使非 root 用户可以运行 kubectl，请运行以下命令， 它们也是 `kubeadm init` 输出的一部分：

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

或者，如果你是 `root` 用户，则可以运行：

```bash
$ export KUBECONFIG=/etc/kubernetes/admin.conf
```

## 3.3 加入集群

在主机 10.252.74.49 上执行 `kubeadm join` 命令，出现下面输出表示加入成功

``` bash
$ kubeadm join 10.252.74.100:6443 --token rpslvk.oll8evpz95qvnk7m \
         --discovery-token-ca-cert-hash sha256:88637abe412a5b07679afd049a534f0f2ae60e3e2fb16c75d438a88273cef346

        [preflight] Running pre-flight checks
        [WARNING FileExisting-socat]: socat not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

默认的token有效期为24小时，当过期之后，该token就不能用了，这时可以使用如下的命令创建token：

``` shell
# 生成一个 token
$ kubeadm token create --print-join-command

# 生成一个永不过期的token
$ kubeadm token create --ttl 0 --print-join-command
```

可以通过 `kubectl get nodes` 查看集群中的节点

``` bash
$ kubectl get nodes
NAME         STATUS     ROLES                  AGE     VERSION
k8s-master   NotReady   control-plane,master   8m39s   v1.21.1
k8s-node-1   NotReady   <none>                 111s    v1.21.1
```

## 3.4 部署网络插件

使用 Weave Net 提供 NetworkPolicy

``` shell
# 下载插件
$ wget -O weave.yml https://cloud.weave.works/k8s/net?k8s-version=1.21.0

# 使用配置文件启动 flannel
$ kubectl apply -f weave.yml

# 查看部署  网络插件进度，直到所有 pod 状态为 Running 表示安装成功
$ kubectl get pods -n kube-system
NAME                                 READY   STATUS    RESTARTS   AGE
coredns-558bd4d5db-2vgdh             1/1     Running   0          3h28m
coredns-558bd4d5db-j5zpm             1/1     Running   0          3h28m
etcd-k8s-master                      1/1     Running   0          3h29m
kube-apiserver-k8s-master            1/1     Running   0          3h29m
kube-controller-manager-k8s-master   1/1     Running   0          3h18m
kube-proxy-dbmft                     1/1     Running   0          3h28m
kube-proxy-vsm7w                     1/1     Running   0          3h28m
kube-scheduler-k8s-master            1/1     Running   0          3h18m
weave-net-sghbj                      2/2     Running   1          3h23m
weave-net-x5xj7                      2/2     Running   1          3h23m

# 再次 get nodes 状态已经是 Ready
$ kubectl get nodes
NAME         STATUS   ROLES                  AGE     VERSION
k8s-master   Ready    control-plane,master   3h32m   v1.21.0
k8s-node-1   Ready    <none>                 3h31m   v1.21.0
```

查看集群健康状况，命令：`kubectl get cs`

``` bash
$ kubectl get cs			# 发现报如下错误：
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS      MESSAGE                                                                                       ERROR
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused
controller-manager   Unhealthy   Get "http://127.0.0.1:10252/healthz": dial tcp 127.0.0.1:10252: connect: connection refused
etcd-0               Healthy     {"health":"true"}
```

``` bash
# 出现这种情况，是 /etc/kubernetes/manifests/ 下的 kube-controller-manager.yaml 和 kube-scheduler.yaml 设置的默认端口是0导致的
# 解决方式是注释掉对应的 port 即可，操作如下：

$ vim /etc/kubernetes/manifests/kube-controller-manager.yaml
```

![image-20210520213929379](../img/image-20210520213929379.png)

``` bash
$ vim /etc/kubernetes/manifests/kube-scheduler.yaml
```

![image-20210520214147092](../img/image-20210520214147092.png)

``` bash
# 重启 kubelet
$ systemctl restart kubelet

# 再次查看集群健康状况
$ kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health":"true"}

$ kubectl cluster-info
Kubernetes control plane is running at https://10.252.74.48:6443
CoreDNS is running at https://10.252.74.48:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

# 4.部署 openfaas 环境

## 4.1 安装 openfaas-cli

安装 openfaas-cli

``` bash
$ curl -sL https://cli.openfaas.com | sh
```

## 4.2 使用 k8s 的 yaml 文件安装 openfaas

### 4.2.1 创建 namespace

创建 namespace

``` bash
$ git clone https://github.com/openfaas/faas-netes
$ cd faas-netes
$ kubectl apply -f namespaces.yml
```

### 4.2.2 创建 admin 的密码

```bash
$ kubectl -n openfaas create secret generic basic-auth \
	--from-literal=basic-auth-user=admin \
	--from-literal=basic-auth-password=admin
```

### 4.2.3 部署openfaas

部署 openfaas

```bash
$ kubectl apply -f ./yaml
```

查看部署的状态，直到所有组件的状态都是 **Running**

``` bash
$ kubectl get pods -n openfaas
NAME                                 READY   STATUS    RESTARTS   AGE
alertmanager-86f8447b8d-92g5f        1/1     Running   0          140m
basic-auth-plugin-78c65cd966-dxc4q   1/1     Running   0          140m
gateway-f79d87f45-6tbpz              2/2     Running   0          140m
nats-6b6564d858-x97gx                1/1     Running   0          140m
prometheus-59fd74885f-sl8z4          1/1     Running   0          140m
queue-worker-5f6cb648db-mfjx5        1/1     Running   0          140m
```

### 4.2.4 设置环境变量

设置环境变量OPENFAAS_URL

```bash
$ export OPENFAAS_URL=http://127.0.0.1:31112
```

### 4.2.5 登录 openfaas

```bash
$ faas-cli login -u admin --password admin
```

### 4.2.6 使用浏览器访问 openfaas

 使用浏览器访问 openfaas 的 UI：http://10.252.74.100:31112/，登录的账号密码都是 admin

![image-20210527155159666](../img/image-20210527155159666.png)

## 4.3 部署 openfaas 函数

现在演示的是基于 dockerHub 镜像仓库，发布 java openfaas 函数。

> 准备：要先创建一个 dockerhub 的账号

### 4.3.1 函数编写

1.创建函数，命令：`faas-cli new <函数名称> --lang <语言类型> -p <docker用户名>` 

``` bash
$ faas-cli new computer-info --lang java11 -p silentao
2021/05/27 14:04:30 No templates found in current directory.
2021/05/27 14:04:30 Attempting to expand templates from https://github.com/openfaas/templates.git
2021/05/27 14:04:33 Fetched 13 template(s) : [csharp dockerfile go java11 java11-vert-x node node12 node14 php7 python python3 python3-debian ruby] from https://github.com/openfaas/templates.git
Folder: computer-info created.
  ___                   _____           ____
 / _ \ _ __   ___ _ __ |  ___|_ _  __ _/ ___|
| | | | '_ \ / _ \ '_ \| |_ / _` |/ _` \___ \
| |_| | |_) |  __/ | | |  _| (_| | (_| |___) |
 \___/| .__/ \___|_| |_|_|  \__,_|\__,_|____/
      |_|


Function created in folder: computer-info
Stack file written: computer-info.yml

Notes:
You have created a function using the java11 template which uses an LTS
version of the OpenJDK.
```

查看当前目录新增了 computer-info 目录和 computer-info.yml 文件。computer-info 目录的结构如下，是个 gradle 工程：

``` shell
computer-info
├── build.gradle
├── gradle
│   └── wrapper
│       ├── gradle-wrapper.jar
│       └── gradle-wrapper.properties
├── gradlew
├── gradlew.bat
├── settings.gradle
└── src
    ├── main
    │   └── java
    │       └── com
    │           └── openfaas
    │               └── function
    │                   └── Handler.java
    └── test
        └── java
            └── HandlerTest.java
```

2.函数编写

修改 build.gradle 文件，引入要用到的依赖

``` java
dependencies {
    // This dependency is exported to consumers, that is to say found on their compile classpath.
    api 'org.apache.commons:commons-math3:3.6.1'

    // This dependency is used internally, and not exposed to consumers on their own compile classpath.
    implementation 'com.google.guava:guava:23.0'

    // Use JUnit test framework
    testImplementation 'junit:junit:4.12'

    compile 'com.openfaas:model:0.1.1'
    compile 'com.openfaas:entrypoint:0.1.0'

    compile 'com.fasterxml.jackson.core:jackson-databind:2.11.0'
    compile 'org.apache.commons:commons-lang3:3.10'
}
```

将 Handler.java 里面的代码修改成自己想要的代码

``` java
package com.openfaas.function;

import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.google.common.collect.Maps;
import com.openfaas.model.IRequest;
import com.openfaas.model.IResponse;
import com.openfaas.model.Response;

import java.net.InetAddress;
import java.net.UnknownHostException;
import java.util.Map;
import java.util.Properties;

public class Handler extends com.openfaas.model.AbstractHandler {

    private ObjectMapper mapper = new ObjectMapper();

    public IResponse Handle(IRequest req) {
        Response res = new Response();

        Map<String, Object> responseBody = Maps.newHashMap();
        try {
            responseBody.put("clientInfo", getRequestBody(req));
            responseBody.put("computerInfo", getComputerInfo());

            responseBody.put("code", 0);
            responseBody.put("message", "success");
        } catch (Exception e) {
            e.printStackTrace();
            responseBody.put("code", 100);
            responseBody.put("message", "failed");
        }

        try {
            res.setContentType("application/json;charset=utf-8");
            res.setBody(mapper.writeValueAsString(responseBody));
        } catch (Exception e) {
            e.printStackTrace();
        }

	    return res;
    }

    /**
     * 获取主机信息
     * @return
     * @throws UnknownHostException
     */
    private Map<String, Object> getComputerInfo() throws UnknownHostException {
        Runtime r = Runtime.getRuntime();
        Properties props = System.getProperties();
        InetAddress addr;
        addr = InetAddress.getLocalHost();
        String ip = addr.getHostAddress();
        Map<String, String> map = System.getenv();
        String userName = map.get("USERNAME");// 获取用户名
        String computerName = map.get("COMPUTERNAME");// 获取计算机名
        String userDomain = map.get("USERDOMAIN");// 获取计算机域名


        Map<String, Object> infoMap = Maps.newHashMap();
        infoMap.put("用户名", userName);
        infoMap.put("计算机名", computerName);
        infoMap.put("计算机域名", userDomain);
        infoMap.put("本地ip地址", ip);
        infoMap.put("本地主机名", addr.getHostName());
        infoMap.put("JVM可以使用的总内存", r.totalMemory());
        infoMap.put("JVM可以使用的剩余内存", r.freeMemory());
        infoMap.put("JVM可以使用的处理器个数", r.availableProcessors());
        infoMap.put("Java的运行环境版本", props.getProperty("java.version"));
        infoMap.put("Java的运行环境供应商", props.getProperty("java.vendor"));
        infoMap.put("Java供应商的URL", props.getProperty("java.vendor.url"));
        infoMap.put("Java的安装路径", props.getProperty("java.home"));
        infoMap.put("Java的虚拟机规范版本", props.getProperty("java.vm.specification.version"));
        infoMap.put("Java的虚拟机规范供应商", props.getProperty("java.vm.specification.vendor"));
        infoMap.put("Java的虚拟机规范名称", props.getProperty("java.vm.specification.name"));
        infoMap.put("Java的虚拟机实现版本", props.getProperty("java.vm.version"));
        infoMap.put("Java的虚拟机实现供应商", props.getProperty("java.vm.vendor"));
        infoMap.put("Java的虚拟机实现名称", props.getProperty("java.vm.name"));
        infoMap.put("Java运行时环境规范版本", props.getProperty("java.specification.version"));
        infoMap.put("Java运行时环境规范供应商", props.getProperty("java.specification.vender"));
        infoMap.put("Java运行时环境规范名称", props.getProperty("java.specification.name"));
        infoMap.put("Java的类格式版本号", props.getProperty("java.class.version"));
        infoMap.put("Java的类路径", props.getProperty("java.class.path"));
        infoMap.put("加载库时搜索的路径列表", props.getProperty("java.library.path"));
        infoMap.put("默认的临时文件路径", props.getProperty("java.io.tmpdir"));
        infoMap.put("一个或多个扩展目录的路径", props.getProperty("java.ext.dirs"));
        infoMap.put("操作系统的名称", props.getProperty("os.name"));
        infoMap.put("操作系统的构架", props.getProperty("os.arch"));
        infoMap.put("操作系统的版本", props.getProperty("os.version"));
        infoMap.put("文件分隔符", props.getProperty("file.separator"));
        infoMap.put("路径分隔符", props.getProperty("path.separator"));
        infoMap.put("行分隔符", props.getProperty("line.separator"));
        infoMap.put("用户的账户名称", props.getProperty("user.name"));
        infoMap.put("用户的主目录", props.getProperty("user.home"));
        infoMap.put("用户的当前工作目录", props.getProperty("user.dir"));

        return infoMap;
    }

    private Map<String, Object> getRequestBody(IRequest req) {
        Map<String, Object> clientInfo = null;

        try {
            clientInfo = mapper.readValue(req.getBody(), new TypeReference<Map<String, Object>>() {});
        } catch (Exception e) {
            e.printStackTrace();
        }

        return clientInfo;
    }
}
```

### 4.3.2 发布函数

将修改好的函数打包成镜像，然后发布到 dockerhub 上。

1.回到 computer-info.yml 文件的目录下然后执行 `faas-cli build -f computer-info.yml`：

``` bash
$ faas-cli build -f computer-info.yml
[0] > Building computer-info.
Clearing temporary build folder: ./build/computer-info/
Preparing: ./computer-info/ build/computer-info/function
Building: computer-info:latest with java11 template. Please wait..
Sending build context to Docker daemon    298kB
Step 1/30 : FROM openjdk:11-jdk-slim as builder
 ---> ded920e5cb06
Step 2/30 : ENV GRADLE_VER=6.1.1
 ---> Using cache
 ---> 0e53f901b30d
Step 3/30 : RUN apt-get update -qqy   && apt-get install -qqy    --no-install-recommends    curl    ca-certificates    unzip
 ---> Using cache
 ---> 55fa1a6db066
Step 4/30 : RUN mkdir -p /opt/ && cd /opt/     && echo "Downloading gradle.."     && curl -sSfL "https://services.gradle.org/distributions/gradle-${GRADLE_VER}-bin.zip" -o gradle-$GRADLE_VER-bin.zip     && unzip gradle-$GRADLE_VER-bin.zip -d /opt/     && rm gradle-$GRADLE_VER-bin.zip
 ---> Using cache
 ---> 1fdca9762b6b
Step 5/30 : ENV GRADLE_HOME=/opt/gradle-$GRADLE_VER/
 ---> Using cache
 ---> 8e3450bc9afc
Step 6/30 : ENV PATH=$PATH:$GRADLE_HOME/bin
 ---> Using cache
 ---> 9f988e3cdba2
Step 7/30 : RUN mkdir -p /home/app/libs
 ---> Using cache
 ---> f1cac1ba326f
Step 8/30 : ENV GRADLE_OPTS="-Dorg.gradle.daemon=false"
 ---> Using cache
 ---> dac1e484f02f
Step 9/30 : WORKDIR /home/app
 ---> Using cache
 ---> 8eebcd554cdd
Step 10/30 : COPY . /home/app/
 ---> 651887c0a9a9
Step 11/30 : RUN gradle build
 ---> Running in 4524a2cdbb41

Welcome to Gradle 6.1.1!

Here are the highlights of this release:
 - Reusable dependency cache
 - Configurable compilation order between Groovy/Kotlin/Java/Scala
 - New sample projects in Gradle's documentation

# 编译过程

# 打包过程

Step 23/30 : WORKDIR /home/app/
 ---> Running in d1d9fabd816a
Removing intermediate container d1d9fabd816a
 ---> 7574fdf11a71
Step 24/30 : ENV upstream_url="http://127.0.0.1:8082"
 ---> Running in cf07869121e9
Removing intermediate container cf07869121e9
 ---> 124653cac060
Step 25/30 : ENV mode="http"
 ---> Running in 8e4eaaf10707
Removing intermediate container 8e4eaaf10707
 ---> a8487a04c649
Step 26/30 : ENV CLASSPATH="/home/app/function-1.0/function-1.0.jar:/home/app/function-1.0/lib/*"
 ---> Running in d5f1a0d7065e
Removing intermediate container d5f1a0d7065e
 ---> 51315283d7c7
Step 27/30 : ENV fprocess="java -XX:+UseContainerSupport com.openfaas.entrypoint.App"
 ---> Running in 16130f11b6fa
Removing intermediate container 16130f11b6fa
 ---> 602952d1b49f
Step 28/30 : EXPOSE 8080
 ---> Running in ea1ee7a22d57
Removing intermediate container ea1ee7a22d57
 ---> 77e466af387d
Step 29/30 : HEALTHCHECK --interval=5s CMD [ -e /tmp/.lock ] || exit 1
 ---> Running in 0e35e4242638
Removing intermediate container 0e35e4242638
 ---> 497ef680ab74
Step 30/30 : CMD ["fwatchdog"]
 ---> Running in b95213b15734
Removing intermediate container b95213b15734
 ---> 05e014335fe2
Successfully built 05e014335fe2
Successfully tagged computer-info:latest
Image: computer-info:latest built.
[0] < Building computer-info done in 52.08s.
[0] Worker done.

Total build time: 52.08s
```

2.将刚刚打包的镜像发布到 dockerhub，命令：`docker push silentao/computer-info:latest`，前提要先登录自己的 dockerhub 账号：

``` bash
docker push silentao/computer-info:latest
The push refers to repository [docker.io/silentao/computer-info]
a086680fc328: Pushed
4b6f817e09e3: Pushed
808cf5358806: Pushed
be9d364a1080: Pushed
91177a374a33: Pushed
bfb60c17c4ea: Pushed
4560f17d7397: Pushed
b865cfc9b0c3: Pushed
02c055ef67f5: Pushed
latest: digest: sha256:79f62f3b7f0f44dc50361ce5462ef841a7cf604c80e376e0f7c9ab45397d52e6 size: 2423
```

登录到 https://hub.docker.com 发现已经有了对应的镜像：

![image-20210527151728598](../img/image-20210527151728598.png)

### 4.3.3 部署函数

1.一切准备就绪，现在将函数部署到 openfaas 平台，执行命令：`faas-cli deploy -f computer-info.yml --gateway http://10.252.74.100:31112`

``` bash
$ faas-cli deploy -f computer-info.yml --gateway http://10.252.74.100:31112
Deploying: computer-info.
WARNING! You are not using an encrypted connection to the gateway, consider using HTTPS.

Deployed. 202 Accepted.
URL: http://10.252.74.100:31112/function/computer-info
```

2.查看 pod 状态，直到对应的 pod 状态为 Runing，命令：`kubectl get pods -n openfaas-fn`

![image-20210527152419780](../img/image-20210527152419780.png)

3.打开 [openfaas](http://10.252.74.101:31112/ui/) 的 web 端，发现函数已经 Ready 了：

![image-20210527152616815](../img/image-20210527152616815.png)

4.访问函数：http://10.252.74.101:31112/function/computer-info，返回以下数据，正是函数执行的结果：

``` json
{
    "code": 0,
    "computerInfo": {
        "Java的虚拟机实现版本": "11.0.11+9",
        "Java的虚拟机规范版本": "11",
        "Java的虚拟机实现供应商": "Oracle Corporation",
        "路径分隔符": ":",
        "Java运行时环境规范名称": "Java Platform API Specification",
        "Java供应商的URL": "https://openjdk.java.net/",
        "JVM可以使用的处理器个数": 1,
        "加载库时搜索的路径列表": "/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib",
        "一个或多个扩展目录的路径": null,
        "操作系统的版本": "3.10.0-1160.el7.x86_64",
        "用户名": null,
        "Java的安装路径": "/usr/local/openjdk-11",
        "用户的当前工作目录": "/home/app",
        "操作系统的构架": "amd64",
        "本地ip地址": "10.32.0.16",
        "JVM可以使用的剩余内存": 44040192,
        "Java的类路径": "/home/app/function-1.0/function-1.0.jar:/home/app/function-1.0/lib/animal-sniffer-annotations-1.14.jar:/home/app/function-1.0/lib/commons-lang3-3.10.jar:/home/app/function-1.0/lib/commons-math3-3.6.1.jar:/home/app/function-1.0/lib/entrypoint-0.1.0.jar:/home/app/function-1.0/lib/error_prone_annotations-2.0.18.jar:/home/app/function-1.0/lib/guava-23.0.jar:/home/app/function-1.0/lib/j2objc-annotations-1.1.jar:/home/app/function-1.0/lib/jackson-annotations-2.11.0.jar:/home/app/function-1.0/lib/jackson-core-2.11.0.jar:/home/app/function-1.0/lib/jackson-databind-2.11.0.jar:/home/app/function-1.0/lib/jsr305-1.3.9.jar:/home/app/function-1.0/lib/model-0.1.1.jar",
        "Java运行时环境规范版本": "11",
        "Java运行时环境规范供应商": null,
        "Java的类格式版本号": "55.0",
        "默认的临时文件路径": "/tmp",
        "计算机域名": null,
        "Java的虚拟机实现名称": "OpenJDK 64-Bit Server VM",
        "Java的虚拟机规范名称": "Java Virtual Machine Specification",
        "用户的主目录": "/home/app",
        "JVM可以使用的总内存": 60817408,
        "Java的虚拟机规范供应商": "Oracle Corporation",
        "操作系统的名称": "Linux",
        "Java的运行环境版本": "11.0.11",
        "文件分隔符": "/",
        "行分隔符": "\n",
        "用户的账户名称": "app",
        "计算机名": null,
        "Java的运行环境供应商": "Oracle Corporation",
        "本地主机名": "computer-info-8b5b9d977-kg27p"
    },
    "clientInfo": null,
    "message": "success"
}
```

### 4.3.4 删除函数

可以通过以下命令来删除函数：

``` bash
$ faas-cli remove -f computer-info.yml
```

# 5.部署 DashBoard

## 5.1 安装 DashBoard

1.下载 yaml

``` bash
$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml
```

2.修改 yaml，service段配置加上 `type: NodePort`

``` bash
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  # 加上这一行，原本是没有的
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard
```

3.启动服务

``` bash
$ kubectl apply -f recommended.yaml
```

4.查看 services，发现对外暴露端口为：32539

``` bash
$ kubectl get svc -n kubernetes-dashboard
NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
dashboard-metrics-scraper   ClusterIP   10.103.203.250   <none>        8000/TCP        29m
kubernetes-dashboard        NodePort    10.104.110.211   <none>        443:32539/TCP   29m
```

5.浏览器访问：https://10.252.74.100:32539，会看到如下页面，接下来要创建账户

![image-20210526151703977](../img/image-20210526151703977.png)

## 5.2 创建账户

1.创建服务账户（serviceaccount）资源

``` bash
$ kubectl create serviceaccount ui-admin -n kube-system

# 查看服务账户
$ kubectl get sa -n kube-system ui-admin
NAME       SECRETS   AGE
ui-admin   1         81m
```

2.创建集群角色绑定

``` bash
# 查看集群角色
$ kubectl get clusterrole -n kube-system cluster-admin
NAME            CREATED AT
cluster-admin   2021-05-25T06:32:03Z

# 创建集群角色绑定
$ kubectl create clusterrolebinding cluster-ui-admin --clusterrole=cluster-admin --serviceaccount=kube-system:ui-admin

# 查看集群角色绑定
$ kubectl get clusterrolebinding cluster-ui-admin
NAME               ROLE                        AGE
cluster-ui-admin   ClusterRole/cluster-admin   83m
```

3.获取 ui-admin 的 token

``` bash
# 以 yaml 的方式打印 ui-admin 服务账户的详细信息
$ kubectl get sa ui-admin -n kube-system -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: "2021-05-26T05:45:05Z"
  name: ui-admin
  namespace: kube-system
  resourceVersion: "107957"
  uid: f7b29183-522d-4876-8dfe-a3df845dc320
secrets:
- name: ui-admin-token-8rkkw

# 生成 token
$ kubectl describe secret ui-admin-token-8rkkw -n kube-system
Name:         ui-admin-token-8rkkw
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: ui-admin
              kubernetes.io/service-account.uid: f7b29183-522d-4876-8dfe-a3df845dc320

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1066 bytes
namespace:  11 bytes
# 这个就是 token
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IlNZMkhibWo0RE1EOEd3eFA5QlB6a1BJWUN3LTJKZWdKQUVQOGVYV216N2sifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ1aS1hZG1pbi10b2tlbi04cmtrdyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ1aS1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImY3YjI5MTgzLTUyMmQtNDg3Ni04ZGZlLWEzZGY4NDVkYzMyMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTp1aS1hZG1pbiJ9.Mzd1Ma0XvnFj-wy2WPyT7AwiGTBnABUu_ZYZsuySx_owRCMIerw5QeOyBqEpF0DDrqB_HuBQyA9P9SIhdDICNI8IXkJ9yFWcnpIGogj3tp7gbNKvRxAOiErmNcojEw23-kLIr5phK9RLuU_Irp49NJDx42uHXhAs9HgSlf1cVdwUNSr45ZMuDeDauT7rul8ROSkuu5wnHljWvm8nbE9Q4glN4D0tOWUr39IY9DhJJdIrSGxS8-rUUELrlo6_0DoWO_pccDgLs44aeR76HEKK7PKA-xUQfv1g2g25xpoe7yGLURWK8lLmqo9Qh9YZ9GrgGdkYwLfwIf4y4Qyfe-PPTQ
```

因为用 token 登录会出现下面的问题，所以不用这种方式登录，用下面生成 Kubeconfig 的方式登录

![image-20210526153023300](../img/image-20210526153023300.png)

4.创建 kubeconfig 配置文件

``` bash
# 第一步
$ kubectl config set-cluster k8s-master --server="https://10.74.252.100:6443" --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --kubeconfig=/tmp/ui-admin.kubeconfig

# 第二步，这里的 token 用上面生成的
$ kubectl config set-credentials ui-admin --token=eyJhbGciOiJSUzI1NiIsImtpZCI6IlNZMkhibWo0RE1EOEd3eFA5QlB6a1BJWUN3LTJKZWdKQUVQOGVYV216N2sifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ1aS1hZG1pbi10b2tlbi04cmtrdyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ1aS1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImY3YjI5MTgzLTUyMmQtNDg3Ni04ZGZlLWEzZGY4NDVkYzMyMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTp1aS1hZG1pbiJ9.Mzd1Ma0XvnFj-wy2WPyT7AwiGTBnABUu_ZYZsuySx_owRCMIerw5QeOyBqEpF0DDrqB_HuBQyA9P9SIhdDICNI8IXkJ9yFWcnpIGogj3tp7gbNKvRxAOiErmNcojEw23-kLIr5phK9RLuU_Irp49NJDx42uHXhAs9HgSlf1cVdwUNSr45ZMuDeDauT7rul8ROSkuu5wnHljWvm8nbE9Q4glN4D0tOWUr39IY9DhJJdIrSGxS8-rUUELrlo6_0DoWO_pccDgLs44aeR76HEKK7PKA-xUQfv1g2g25xpoe7yGLURWK8lLmqo9Qh9YZ9GrgGdkYwLfwIf4y4Qyfe-PPTQ --kubeconfig=/tmp/ui-admin.kubeconfig

# 第三步
$ kubectl config set-context ui-admin@k8s-master --cluster=k8s-master --user=ui-admin --kubeconfig=/tmp/ui-admin.kubeconfig

# 第四步
$ kubectl config use-context ui-admin@k8s-master --kubeconfig=/tmp/ui-admin.kubeconfig

# 第五步，输出 kubeconfig 配置
$ kubectl config view --kubeconfig=/tmp/ui-admin.kubeconfig
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://10.74.252.100:6443
  name: k8s-master
contexts:
- context:
    cluster: k8s-master
    user: ui-admin
  name: ui-admin@k8s-master
current-context: ui-admin@k8s-master
kind: Config
preferences: {}
users:
- name: ui-admin
  user:
    # 这里的 token 换成上面生成的 token 即可，然后保存成一个 yaml 文件
    token: REDACTED
```

完整的 token.yaml 文件：

``` yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://10.74.252.100:6443
  name: k8s-master
contexts:
- context:
    cluster: k8s-master
    user: ui-admin
  name: ui-admin@k8s-master
current-context: ui-admin@k8s-master
kind: Config
preferences: {}
users:
- name: ui-admin
  user:
    token: eyJhbGciOiJSUzI1NiIsImtpZCI6IlNZMkhibWo0RE1EOEd3eFA5QlB6a1BJWUN3LTJKZWdKQUVQOGVYV216N2sifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ1aS1hZG1pbi10b2tlbi04cmtrdyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ1aS1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImY3YjI5MTgzLTUyMmQtNDg3Ni04ZGZlLWEzZGY4NDVkYzMyMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTp1aS1hZG1pbiJ9.Mzd1Ma0XvnFj-wy2WPyT7AwiGTBnABUu_ZYZsuySx_owRCMIerw5QeOyBqEpF0DDrqB_HuBQyA9P9SIhdDICNI8IXkJ9yFWcnpIGogj3tp7gbNKvRxAOiErmNcojEw23-kLIr5phK9RLuU_Irp49NJDx42uHXhAs9HgSlf1cVdwUNSr45ZMuDeDauT7rul8ROSkuu5wnHljWvm8nbE9Q4glN4D0tOWUr39IY9DhJJdIrSGxS8-rUUELrlo6_0DoWO_pccDgLs44aeR76HEKK7PKA-xUQfv1g2g25xpoe7yGLURWK8lLmqo9Qh9YZ9GrgGdkYwLfwIf4y4Qyfe-PPTQ
```

## 5.3 登录 Dashboard

选择 Kubeconfig 登录方式，然后上传 token.yaml 文件，点登录就好了

![image-20210526153914564](../img/image-20210526153914564.png)

登录后的界面是这样的：

![image-20210526154009891](../img/image-20210526154009891.png)

